"""CTE Infrastructure for Population-Scale FHIRPath Execution.

This module implements the Common Table Expression (CTE) infrastructure that transforms
SQL fragments from the translator (PEP-003) into executable, monolithic SQL queries
optimized for population-scale healthcare analytics.

The CTE infrastructure consists of two primary components:

1. **CTEBuilder**: Wraps `SQLFragment` objects in CTE structures, handling array UNNEST
   operations and generating unique CTE names with dependency tracking.

2. **CTEAssembler**: Combines CTE structures into monolithic SQL queries with
   dependency-ordered WITH clauses and final SELECT statements.

This infrastructure fills the critical gap between the AST-to-SQL Translator (PEP-003)
and database execution, enabling FHIRPath expressions like `Patient.name.given` to
properly flatten FHIR arrays and return population-scale results.

Design Principles:
    - **Separation of Concerns**: Translation logic separate from CTE organization
    - **Population-First Design**: Default to population-scale queries
    - **Database-Agnostic**: CTE structure independent of database syntax
    - **Incremental Complexity**: MVP focuses on essential functionality

Architecture Integration:
    ```
    PEP-003 Translator
        ↓ List[SQLFragment]
    CTEBuilder
        ↓ List[CTE]
    CTEAssembler
        ↓ str (Complete SQL)
    Database Execution
    ```

Example Usage:
    Basic CTE creation and assembly:

    >>> from fhir4ds.fhirpath.sql.cte import CTE, CTEBuilder, CTEAssembler
    >>> from fhir4ds.dialects.duckdb import DuckDBDialect
    >>>
    >>> # Create a simple CTE
    >>> cte = CTE(
    ...     name="cte_1",
    ...     query="SELECT id, json_extract(resource, '$.name') as name FROM patient_resources",
    ...     depends_on=[]
    ... )
    >>> print(cte.name)
    cte_1

    Array flattening with UNNEST:

    >>> # Create CTE chain for Patient.name.given
    >>> cte_1 = CTE(
    ...     name="cte_1",
    ...     query="SELECT id, json_extract(resource, '$.name') as names FROM patient_resources",
    ...     depends_on=[]
    ... )
    >>> cte_2 = CTE(
    ...     name="cte_2",
    ...     query="SELECT cte_1.id, name_item FROM cte_1, LATERAL UNNEST(cte_1.names) AS name_item",
    ...     depends_on=["cte_1"],
    ...     requires_unnest=True
    ... )
    >>> print(cte_2.requires_unnest)
    True

Module: fhir4ds.fhirpath.sql.cte
PEP: PEP-004 - CTE Infrastructure for Population-Scale FHIRPath Execution
Created: 2025-10-19
Author: FHIR4DS Development Team
"""

from dataclasses import dataclass, field
import inspect
import heapq
import logging
import re
from typing import Any, Dict, List, Optional, Set

# Import SQLFragment for type hints (will be used by CTEBuilder)
from fhir4ds.fhirpath.sql.fragments import SQLFragment
from fhir4ds.dialects.base import DatabaseDialect

logger = logging.getLogger(__name__)


@dataclass
class CTE:
    """Represents a Common Table Expression in SQL.

    CTE is the fundamental data structure for the CTE infrastructure. Each CTE
    encapsulates a single SQL operation wrapped in a WITH clause structure,
    along with metadata needed for dependency resolution and query assembly.

    CTEs are generated by CTEBuilder from SQLFragment objects and assembled by
    CTEAssembler into complete monolithic SQL queries.

    Attributes:
        name: Unique CTE name (e.g., "cte_1", "name_unnest"). Used for referencing
            this CTE in subsequent CTEs and in the final SELECT statement. Names
            should be unique within a single query and follow SQL identifier rules.

        query: The SELECT statement for this CTE. This is the actual SQL code that
            defines the CTE's result set. Should be a complete SELECT statement
            without the surrounding "WITH cte_name AS (...)" wrapper.

        depends_on: List of CTE names that this CTE depends on. Used for topological
            sorting to ensure CTEs are defined before they're referenced. An empty
            list indicates this CTE has no dependencies (typically the first CTE
            that queries the base resource table).

        requires_unnest: Flag indicating whether this CTE contains LATERAL UNNEST
            operations for array flattening. True for CTEs that unnest FHIR arrays
            (e.g., Patient.name -> individual name records). Used by CTEBuilder to
            apply appropriate wrapping logic.

        source_fragment: Optional reference to the original SQLFragment that generated
            this CTE. Preserved for debugging purposes, allowing tracing back from
            CTE to original FHIRPath expression. None if CTE was created without
            a source fragment.

        source_expression: Optional string copy of the SQL fragment expression that
            produced this CTE. Maintained separately from ``source_fragment`` so that
            high-level tooling can access the raw SQL text even when the fragment
            object is unavailable (for example after serialization).

        metadata: Extensible dictionary for additional metadata. Allows future
            enhancements without breaking changes to the dataclass structure.
            Potential uses include performance hints, optimization flags, and
            debugging information.

    Design Decisions:
        1. **Immutability**: CTE uses frozen=False (mutable) to allow post-creation
           updates if needed during assembly (e.g., dependency updates). However,
           typical usage should treat CTEs as immutable once created.

        2. **Metadata Dictionary**: Provides extensibility without structural changes.
           Future optimizations can add metadata without modifying the dataclass.

        3. **Lightweight References**: source_fragment is Optional to avoid circular
           dependencies and keep memory overhead low.

        4. **String-Based Dependencies**: depends_on uses string names rather than
           CTE references to avoid circular dependencies and simplify topological
           sorting.

    Example:
        Simple path extraction CTE:

        >>> cte = CTE(
        ...     name="cte_1",
        ...     query=("SELECT id, json_extract(resource, '$.birthDate') as birth_date "
        ...            "FROM patient_resources"),
        ...     depends_on=[]
        ... )
        >>> print(cte.name)
        cte_1
        >>> print(cte.requires_unnest)
        False

        Array flattening CTE with UNNEST:

        >>> cte = CTE(
        ...     name="cte_2",
        ...     query='''
        ...         SELECT cte_1.id, name_item
        ...         FROM cte_1, LATERAL UNNEST(cte_1.names) AS name_item
        ...     ''',
        ...     depends_on=["cte_1"],
        ...     requires_unnest=True
        ... )
        >>> print(cte.requires_unnest)
        True
        >>> print(cte.depends_on)
        ['cte_1']

        CTE with metadata:

        >>> fragment = SQLFragment(expression="SELECT * FROM patient", source_table="patient")
        >>> cte = CTE(
        ...     name="cte_1",
        ...     query="SELECT id, resource FROM patient_resources",
        ...     depends_on=[],
        ...     source_fragment=fragment,
        ...     metadata={"estimated_rows": 1000000, "resource_type": "Patient"}
        ... )
        >>> print(cte.metadata["resource_type"])
        Patient

    Future Considerations:
        - Optimization metadata (predicate pushdown hints, CTE merging candidates)
        - Type information (expected result schema, FHIR type mappings)
        - Performance metadata (estimated row counts, query cost estimates)
        - Debugging information (source FHIRPath expression, AST node reference)

    See Also:
        - CTEBuilder: Generates CTE instances from SQLFragments
        - CTEAssembler: Combines CTEs into complete SQL queries
        - SQLFragment: Input to CTEBuilder, source of CTE data
        - PEP-004: Complete specification for CTE infrastructure
    """

    name: str
    query: str
    depends_on: List[str] = field(default_factory=list)
    requires_unnest: bool = False
    source_fragment: Optional[SQLFragment] = None
    source_expression: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self) -> None:
        """Validate CTE after initialization.

        Performs basic validation to ensure the CTE is well-formed:
        - Name must be non-empty string and valid SQL identifier
        - Query must be non-empty string
        - depends_on must be a list (even if empty)
        - metadata must be a dictionary (even if empty)

        Raises:
            ValueError: If validation fails with descriptive error message
        """
        if not self.name or not isinstance(self.name, str):
            raise ValueError("name must be a non-empty string")

        if not self.query or not isinstance(self.query, str):
            raise ValueError("query must be a non-empty string")

        if not isinstance(self.depends_on, list):
            raise ValueError("depends_on must be a list")

        if not isinstance(self.metadata, dict):
            raise ValueError("metadata must be a dictionary")

        if self.source_expression is not None and not isinstance(self.source_expression, str):
            raise ValueError("source_expression must be a string when provided")

        # Validate CTE name is a valid SQL identifier (basic check)
        if not self.name.replace("_", "").isalnum():
            raise ValueError(
                f"name '{self.name}' must be a valid SQL identifier "
                "(alphanumeric characters and underscores only)"
            )

    def add_dependency(self, dependency: str) -> None:
        """Add a CTE dependency to this CTE.

        Args:
            dependency: Name of the CTE that this CTE depends on. Must be a valid
                CTE name that will be defined before this CTE in the WITH clause.

        Example:
            >>> cte = CTE(name="cte_2", query="SELECT * FROM cte_1", depends_on=[])
            >>> cte.add_dependency("cte_1")
            >>> print(cte.depends_on)
            ['cte_1']
        """
        if dependency not in self.depends_on:
            self.depends_on.append(dependency)

    def set_metadata(self, key: str, value: Any) -> None:
        """Set a metadata value for extensibility.

        Args:
            key: Metadata key (string identifier)
            value: Metadata value (can be any type)

        Example:
            >>> cte = CTE(name="cte_1", query="SELECT * FROM patient", depends_on=[])
            >>> cte.set_metadata("estimated_rows", 1000000)
            >>> cte.set_metadata("optimization_hint", "use_index")
            >>> print(cte.metadata)
            {'estimated_rows': 1000000, 'optimization_hint': 'use_index'}
        """
        self.metadata[key] = value

    def get_metadata(self, key: str, default: Any = None) -> Any:
        """Get a metadata value with optional default.

        Args:
            key: Metadata key to retrieve
            default: Default value if key not found (defaults to None)

        Returns:
            Metadata value or default if not found

        Example:
            >>> cte = CTE(name="cte_1", query="SELECT * FROM patient", depends_on=[])
            >>> cte.set_metadata("rows", 1000)
            >>> print(cte.get_metadata("rows"))
            1000
            >>> print(cte.get_metadata("missing_key", "not_found"))
            not_found
        """
        return self.metadata.get(key, default)

    def rename_cte(self, old_name: str, new_name: str) -> None:
        """Rename this CTE and update internal references.

        This method updates both the CTE name and any references to the old name
        within the CTE's query. It also updates the depends_on list if it contains
        the old name.

        Args:
            old_name: Current name of the CTE
            new_name: New name to assign to the CTE

        Example:
            >>> cte = CTE(
            ...     name="cte_1",
            ...     query="SELECT id FROM cte_1 WHERE value > 5",
            ...     depends_on=[]
            ... )
            >>> cte.rename_cte("cte_1", "where_eval_1_1")
            >>> print(cte.name)
            where_eval_1_1
            >>> print(cte.query)
            SELECT id FROM where_eval_1_1 WHERE value > 5
        """
        if self.name != old_name:
            raise ValueError(
                f"CTE name mismatch: expected '{old_name}', got '{self.name}'"
            )

        # Update the CTE name
        self.name = new_name

        # Update references in the query (replace old CTE name with new name)
        # Use word boundaries to avoid partial matches (e.g., cte_1 -> cte_10)
        import re
        # Pattern matches: FROM old_name, JOIN old_name, old_name.column, etc.
        # Using word boundary \b to ensure we match complete identifiers
        pattern = r'\b' + re.escape(old_name) + r'\b'
        self.query = re.sub(pattern, new_name, self.query)

        # Update depends_on list if it contains the old name
        if old_name in self.depends_on:
            self.depends_on = [new_name if dep == old_name else dep for dep in self.depends_on]

    @staticmethod
    def rename_cte_chain(ctes: List['CTE'], name_mapping: Dict[str, str]) -> None:
        """Rename a chain of CTEs and update all cross-references.

        This method renames multiple CTEs in a chain and updates all references
        between them. It handles the case where CTE_2 references CTE_1, and both
        are being renamed.

        Args:
            ctes: List of CTEs to rename (in order)
            name_mapping: Dictionary mapping old CTE names to new names

        Example:
            >>> cte1 = CTE(name="cte_1", query="SELECT id FROM resource", depends_on=[])
            >>> cte2 = CTE(name="cte_2", query="SELECT id FROM cte_1", depends_on=["cte_1"])
            >>> CTE.rename_cte_chain([cte1, cte2], {"cte_1": "where_eval_1_1", "cte_2": "where_eval_1_2"})
            >>> print(cte1.name)
            where_eval_1_1
            >>> print(cte2.query)
            SELECT id FROM where_eval_1_1
        """
        import re

        # First pass: rename all CTE names
        for cte in ctes:
            if cte.name in name_mapping:
                old_name = cte.name
                new_name = name_mapping[old_name]
                cte.name = new_name

        # Second pass: update all references in queries and depends_on
        for cte in ctes:
            # Update references in the query
            for old_name, new_name in name_mapping.items():
                pattern = r'\b' + re.escape(old_name) + r'\b'
                cte.query = re.sub(pattern, new_name, cte.query)

            # Update depends_on list
            if cte.depends_on:
                updated_deps = []
                for dep in cte.depends_on:
                    if dep in name_mapping:
                        updated_deps.append(name_mapping[dep])
                    else:
                        updated_deps.append(dep)
                cte.depends_on = updated_deps


class CTEManager:
    """Manages CTE generation from SQL fragments to final SQL query.

    This class combines the functionality of CTEBuilder and CTEAssembler into a
    single cohesive unit, eliminating the need for intermediate CTE list handoff
    between separate components. The unified approach ensures full context is
    available throughout the translation process.

    Key responsibilities:
        - Convert SQL fragments to CTE structures (from CTEBuilder)
        - Assemble CTEs into complete SQL queries (from CTEAssembler)
        - Generate stable, unique CTE names
        - Wrap fragment expressions in population-friendly SELECT statements
        - Preserve dependency information and perform topological sorting
        - Handle array UNNEST operations with ordering preservation
        - Generate WITH clauses and final SELECT statements

    Only syntax-specific behavior is delegated to the injected dialect. Business
    logic such as dependency tracking and population-first wrapping remains in
    this class to uphold the unified architecture guidelines.

    Example:
        >>> from fhir4ds.dialects.duckdb import DuckDBDialect
        >>> from fhir4ds.fhirpath.sql.fragments import SQLFragment
        >>> manager = CTEManager(DuckDBDialect())
        >>> fragment = SQLFragment(
        ...     expression="json_extract(resource, '$.birthDate')",
        ...     source_table="resource",
        ...     metadata={"result_alias": "birth_date"},
        ... )
        >>> sql = manager.generate_sql([fragment])
        >>> "WITH" in sql and "SELECT" in sql
        True

    Architecture Integration:
        ```
        PEP-003 Translator
            ↓ List[SQLFragment]
        CTEManager.generate_sql()
            ↓ str (Complete SQL)
        Database Execution
        ```
    """

    def __init__(self, dialect: DatabaseDialect) -> None:
        """Initialize the manager with a database dialect.

        Args:
            dialect: Concrete `DatabaseDialect` implementation used for any
                database-specific SQL syntax.

        Raises:
            ValueError: If a dialect instance is not provided.
        """
        if dialect is None:
            raise ValueError("dialect must be provided for CTEManager")

        self.dialect = dialect
        self.cte_counter: int = 0

    def generate_sql(self, fragments: List[SQLFragment]) -> str:
        """Convert SQL fragments to complete SQL query.

        This is the main entry point. It:
        1. Converts fragments to CTEs
        2. Orders CTEs by dependencies
        3. Generates the WITH clause
        4. Generates the final SELECT

        Args:
            fragments: Ordered SQL fragments from translator

        Returns:
            Complete SQL query string ready for execution

        Raises:
            ValueError: If the fragment list is empty or contains invalid entries.
        """
        if not fragments:
            raise ValueError("At least one fragment required")

        # Reset counter for each query
        self.cte_counter = 0

        # Build CTEs from fragments (formerly CTEBuilder)
        ctes = self._build_cte_chain(fragments)

        # Assemble into SQL (formerly CTEAssembler)
        sql = self._assemble_query(ctes)

        return sql

    # === Methods from CTEBuilder ===

    def _build_cte_chain(self, fragments: List[SQLFragment]) -> List[CTE]:
        """Convert a sequence of SQL fragments into an ordered CTE chain.

        The builder processes fragments in order, generating a new CTE for each
        fragment and preserving the sequential dependency chain required by the
        assembler. Each generated CTE references the previous CTE name (if any),
        enabling downstream topological sorting without embedding business logic
        in dialect classes.

        ARRAY ORDERING FIX (SP-020-DEBUG):
            For UNNEST operations, adds ROW_NUMBER() tracking to preserve array
            element ordering across nested LATERAL UNNEST operations. Without this,
            DuckDB loses ordering causing compliance test failures.

        SP-110-006 FIX: Track available columns from previous CTEs to enable
            proper column propagation. When a fragment references a column from
            a previous UNNEST CTE (like "name_item"), ensure that column is
            available in subsequent CTEs.

        Args:
            fragments: Ordered SQL fragments emitted by the translator.

        Returns:
            List of `CTE` instances ready for assembly.
        """
        if not fragments:
            return []

        ctes: List[CTE] = []
        previous_cte_name: Optional[str] = None
        ordering_columns: List[str] = []  # Track ordering columns for UNNEST operations
        available_columns: Dict[str, Set[str]] = {}  # Track columns available in each CTE

        for fragment in fragments:
            cte = self._fragment_to_cte(fragment, previous_cte_name, ordering_columns, available_columns)
            ctes.append(cte)
            previous_cte_name = cte.name

            # Track what columns this CTE makes available
            available_columns[cte.name] = self._extract_cte_columns(cte)

            # If this CTE added an ordering column, track it for subsequent CTEs
            if "order_column" in cte.metadata:
                ordering_columns.append(cte.metadata["order_column"])
            else:
                # SP-022-019: If a CTE generates a pre-built SELECT (like select())
                # that doesn't propagate ordering columns, clear them. Otherwise,
                # subsequent CTEs would try to reference columns that don't exist.
                expression = fragment.expression.strip()
                if expression.upper().startswith("SELECT"):
                    ordering_columns.clear()

        return ctes

    def _fragment_to_cte(
        self,
        fragment: SQLFragment,
        previous_cte: Optional[str],
        ordering_columns: Optional[List[str]] = None,
        available_columns: Optional[Dict[str, Set[str]]] = None,
    ) -> CTE:
        """Convert a single SQL fragment into a CTE instance.

        Generates a unique CTE name, wraps the fragment in the appropriate SQL
        template (simple SELECT or UNNEST placeholder), and carries forward any
        dependency metadata so later stages can perform topological sorting.

        Args:
            fragment: The SQL fragment to wrap.
            previous_cte: Name of the previously generated CTE, if any. When
                provided it will be appended to the dependency list to preserve
                the sequential execution order.
            ordering_columns: List of ordering column names from previous UNNEST
                operations, used for ROW_NUMBER() PARTITION BY clauses.

        Returns:
            A fully-populated `CTE` instance.

        Raises:
            ValueError: If the fragment cannot be converted into a query (for
                example when no source table can be resolved).
        """
        cte_name = self._generate_cte_name(fragment)
        source_table = previous_cte or fragment.source_table

        if not source_table:
            raise ValueError(
                "CTEManager requires a source table to wrap fragments; "
                "neither previous CTE nor fragment.source_table was provided."
            )

        # SP-110-006: Auto-detect columns from previous CTEs that should be propagated
        # When a fragment references "resource" directly but there's a previous CTE
        # with _item columns, preserve those columns in the SELECT clause.
        # This fixes cases like: Patient.name.subsetOf($this.name.first())
        # where fragment 1 ($this.name) incorrectly references resource instead of cte_1
        auto_preserve_columns: Set[str] = set()
        if previous_cte and available_columns and previous_cte in available_columns:
            prev_columns = available_columns[previous_cte]
            # Find _item columns from the previous CTE
            item_cols = {col for col in prev_columns if col.endswith('_item')}
            if item_cols:
                logger.debug(
                    f"SP-110-006: Auto-preserving columns from {previous_cte}: {item_cols}"
                )
                auto_preserve_columns = item_cols

        if fragment.requires_unnest:
            query, order_column = self._wrap_unnest_query(
                fragment, source_table, cte_name, ordering_columns or []
            )
        else:
            query = self._wrap_simple_query(
                fragment, source_table, ordering_columns or [], auto_preserve_columns
            )
            order_column = None

        dependencies: List[str] = []
        if previous_cte:
            dependencies.append(previous_cte)
        dependencies.extend(fragment.dependencies)

        # Preserve dependency ordering while avoiding duplicates
        seen: Dict[str, None] = {}
        ordered_dependencies = []
        for dep in dependencies:
            if dep and dep not in seen:
                seen[dep] = None
                ordered_dependencies.append(dep)

        # Copy metadata and add ordering column if present
        metadata = dict(fragment.metadata)
        if order_column:
            metadata["order_column"] = order_column

        return CTE(
            name=cte_name,
            query=query,
            depends_on=ordered_dependencies,
            requires_unnest=fragment.requires_unnest,
            source_fragment=fragment,
            source_expression=fragment.expression,
            metadata=metadata,
        )

    def _generate_cte_name(self, fragment: SQLFragment) -> str:
        """Generate a unique CTE name for the provided fragment."""
        self.cte_counter += 1
        return f"cte_{self.cte_counter}"

    def _extract_cte_columns(self, cte: CTE) -> Set[str]:
        """Extract the column names that a CTE outputs.

        This is used for SP-110-006 to track which columns are available
        in each CTE for proper propagation.

        Args:
            cte: The CTE to analyze

        Returns:
            Set of column names that this CTE outputs
        """
        import re
        columns = set()

        # Parse the SELECT clause to find column names
        # Look for patterns like:
        # - SELECT col1, col2, ...
        # - SELECT expr AS alias, ...
        # - SELECT table.col, ...
        query = cte.query.strip()

        # Find the SELECT clause (between SELECT and FROM)
        select_match = re.search(r'SELECT\s+(.+?)\s+FROM', query, re.IGNORECASE | re.DOTALL)
        if not select_match:
            return columns

        select_clause = select_match.group(1).strip()

        # Split by comma (but not within parentheses)
        # This is a simple split; for production, use a proper SQL parser
        parts = []
        paren_depth = 0
        current_part = []
        for char in select_clause:
            if char == '(':
                paren_depth += 1
                current_part.append(char)
            elif char == ')':
                paren_depth -= 1
                current_part.append(char)
            elif char == ',' and paren_depth == 0:
                parts.append(''.join(current_part).strip())
                current_part = []
            else:
                current_part.append(char)
        if current_part:
            parts.append(''.join(current_part).strip())

        # Extract column names from each part
        for part in parts:
            # Check for AS alias
            as_match = re.search(r'\s+AS\s+(\w+)$', part, re.IGNORECASE)
            if as_match:
                columns.add(as_match.group(1))
            else:
                # No AS alias - use the last identifier
                # This handles: table.col, col, function(...)
                identifiers = re.findall(r'\b(\w+)\b', part)
                if identifiers:
                    columns.add(identifiers[-1])

        logger.debug(f"CTE {cte.name} outputs columns: {columns}")
        return columns

    def _wrap_simple_query(
        self, fragment: SQLFragment, source_table: str, ordering_columns: List[str],
        auto_preserve_columns: Optional[Set[str]] = None
    ) -> str:
        """Wrap a non-UNNEST fragment in a population-first SELECT statement.

        For fragments that produce scalar expressions (e.g., JSON extraction,
        arithmetic, boolean predicates), this method generates a SELECT statement
        that preserves the patient-level `id` column while projecting the fragment
        expression as `result`. When a fragment already contains a full SELECT
        statement it is returned unchanged.

        SP-022-004 FIX: When a fragment has a "filter" in metadata, adds a WHERE
        clause to filter rows (used by first()/last()/skip()/take() on unnested
        collections).

        SP-105 FIX: When a fragment has "subset_filter" and the expression is a
        simple column reference (like "name_item"), preserve that column in the
        SELECT clause so subsequent operations can access it. This fixes the
        "Referenced column 'name_item' not found" error after take()/first()/last().

        SP-110-006 FIX: For repeat() function, extract and preserve repeat_elem_N
        columns from the WITH RECURSIVE subquery. The repeat() function returns a
        complete RECURSIVE CTE expression, and we need to extract the element column
        from it so subsequent operations (like UNNEST) can access it.

        Args:
            fragment: SQL fragment to wrap.
            source_table: Table or CTE providing input rows for this fragment.
            ordering_columns: Ordering columns from previous UNNESTs to preserve.

        Returns:
            SQL SELECT statement representing the fragment as a CTE query.
        """
        expression = fragment.expression.strip()

        if not expression:
            raise ValueError("SQLFragment expression cannot be empty when wrapping CTE")

        # If the translator already provided a full SELECT statement, respect it.
        # SP-109-004: If it contains <<SOURCE_TABLE>>, substitute the actual source_table
        if expression.upper().startswith("SELECT"):
            if "<<SOURCE_TABLE>>" in expression:
                # Validate source_table is a safe SQL identifier before substitution
                # This prevents SQL injection by ensuring only valid table names are used
                import re
                if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', source_table):
                    raise ValueError(f"Invalid table name for substitution: {source_table}")
                # Substitute <<SOURCE_TABLE>> with the actual source_table
                # This handles both "<<SOURCE_TABLE>>" and "<<SOURCE_TABLE>>.result"
                original_expression = expression
                expression = expression.replace('<<SOURCE_TABLE>>', source_table)
                # Verify substitution was successful
                if "<<SOURCE_TABLE>>" in expression:
                    raise ValueError(f"Failed to substitute <<SOURCE_TABLE>> placeholder in expression: {original_expression[:200]}")
            return expression

        result_alias = fragment.metadata.get("result_alias", "result")

        # SP-110-006: Check if this is a repeat() function result that needs special handling
        # repeat() returns a complete RECURSIVE CTE expression like:
        # (WITH RECURSIVE repeat_enum_0 AS (...), repeat_recursive_0 AS (...) SELECT COALESCE(...) FROM (...))
        # We need to check if it contains repeat_elem_N and extract that column properly
        is_repeat_expression = False
        repeat_elem_column = None
        if "repeat_elem_" in expression and "WITH RECURSIVE" in expression.upper():
            # Extract the repeat_elem_N column name using regex
            import re
            repeat_match = re.search(r'(repeat_elem_\d+)', expression)
            if repeat_match:
                is_repeat_expression = True
                repeat_elem_column = repeat_match.group(1)
                logger.info(
                    f"SP-110-006: Detected repeat() expression with element column '{repeat_elem_column}'"
                )

        # SP-105: Check if this fragment references a collection item column that needs to be preserved
        # This happens when take()/first()/last()/skip() operate on unnested collections
        # The expression will be something like "name_item" (a column reference, not a complex expression)
        subset_filter = fragment.metadata.get("subset_filter")
        preserve_item_column = False
        item_column_name = None

        if subset_filter:
            # Check if the expression is a simple column reference (not a function call or complex expression)
            # Simple column references: name_item, cte_1_item, etc.
            # Complex expressions: json_extract(...), CASE WHEN..., etc.
            import re
            is_simple_column_ref = (
                expression and
                not expression.upper().startswith("SELECT") and
                not expression.upper().startswith("CASE") and
                not expression.upper().startswith("JSON_") and
                not expression.upper().startswith("COALESCE") and
                not expression.upper().startswith("CAST") and
                not "(" in expression and  # No function calls
                not " " in expression     # No spaces (single word)
            )

            if is_simple_column_ref:
                # This looks like a column reference - preserve it in the SELECT
                preserve_item_column = True
                item_column_name = expression
                logger.info(
                    f"SP-105: Preserving collection item column '{item_column_name}' "
                    f"for subset_filter '{subset_filter}'"
                )

        # SP-025-003 FIX: When source_table is an actual resource type (not "resource" placeholder),
        # qualify bare "resource" references in fragment expressions.
        # This handles cases like Patient.name.empty() where SQL generator updates source_table to "Patient",
        # but fragment expressions still contain json_extract(resource, ...) which should be
        # json_extract(Patient.resource, ...) to reference the correct column.
        if source_table and source_table != "resource" and not source_table.startswith("cte_"):
            # Pattern matches 'resource' as a standalone column reference in json_* functions:
            # - Not preceded by a dot (would be table.resource already)
            # - Not preceded by alphanumeric (would be part of another name)
            # - Followed by comma, closing paren, or end of string
            # This handles: json_extract(resource, ...), json_extract_string(resource, ...), etc.
            expression = re.sub(
                r'(?<![\w.])resource(?=[\s,)])',
                f'{source_table}.resource',
                expression
            )

        # FIX: When source_table is not "resource" and not a CTE, it means we're
        # selecting from an actual resource table (e.g., "Patient", "Observation").
        # These tables have a "resource" column containing JSON data, not an "id" column.
        # We need to extract the id from the resource column.
        if source_table and source_table != "resource" and not source_table.startswith("cte_"):
            # Use dialect's extract_json_string method to get id from resource column
            id_column = fragment.metadata.get("id_column",
                f"{self.dialect.extract_json_string(f'{source_table}.resource', '$.id')}")
        else:
            # Original logic for "resource" placeholder or CTEs
            id_column = fragment.metadata.get("id_column", f"{source_table}.id")

        # Build column list: id, resource (if available), ordering columns, result
        columns = [id_column]

        # FIX: Include resource column to propagate it through CTE chain
        # This fixes "Referenced column 'resource' not found" errors
        if source_table == "resource":
            # First CTE selecting from resource table
            columns.append("resource")
        elif source_table and not source_table.startswith("cte_"):
            # Selecting from some other table (not a CTE)
            columns.append(f"{source_table}.resource")
        else:
            # Selecting from a previous CTE - pass through resource if it has it
            # We include it optimistically; if it doesn't exist, query will fail
            # which is fine - it means translator generated incorrect SQL
            columns.append(f"{source_table}.resource")

        # SP-108-003: Check if order columns should be excluded from SELECT
        # Functions like all() aggregate across all elements, so order columns
        # should not be included in SELECT or GROUP BY
        exclude_order = fragment.metadata.get("exclude_order_from_group_by", False)
        if not exclude_order:
            columns.extend(ordering_columns)

        # SP-105 Phase 2: Propagate _item columns from source CTEs throughout the chain
        # This fixes "column not found" errors when combine/exclude reference columns
        # that were available in earlier CTEs but not propagated forward
        # SP-108-003: Skip item column propagation if exclude_order is True
        # SP-110: Exclude lambda-scoped columns (exists_, select_, repeat_) from propagation
        # These columns are only defined within subqueries and don't exist in the parent CTE
        # SP-110 Phase 2: Extended CTE column propagation for name_item, cte_N_item patterns
        # SP-110 Phase 3: Handle qualified column references (cte_2.name_item) in subsetOf/supersetOf
        propagated_item_columns = set()
        qualified_item_columns = set()  # SP-110 Phase 3: Track fully qualified column references
        if source_table.startswith("cte_") and not exclude_order:
            # Check if any previous CTEs had _item columns that should be propagated
            # We need to look at what columns are available in the source CTE
            # For now, we conservatively preserve columns that are referenced in the expression
            import re

            # SP-110 Phase 3: First, find qualified column references (cte_X.column_name)
            # These must be preserved as-is without re-qualification
            qualified_refs = re.findall(r'\b(cte_\d+)\.(\w+_item)\b', expression)
            for cte_name, col_name in qualified_refs:
                qualified_ref = f"{cte_name}.{col_name}"
                qualified_item_columns.add(qualified_ref)
                logger.debug(
                    f"SP-110 Phase 3: Found qualified column reference '{qualified_ref}'"
                )

            # Find all _item column references in the expression
            # Match both qualified (cte_X.name_item) and unqualified (name_item) references
            item_refs = re.findall(r'\b(\w+_item)\b', expression)

            # SP-110 Phase 2: Also check for repeat_elem_N patterns from repeat() function
            repeat_refs = re.findall(r'\b(repeat_elem_\d+)\b', expression)
            item_refs.extend(repeat_refs)

            # SP-110 Phase 2: Check for property access patterns like 'given', 'use', 'family'
            # These are typically accessed from _item columns in lambda contexts
            property_refs = re.findall(r'\b(given|use|family|period|value|system|code|display)\b', expression)

            for ref in item_refs:
                # SP-110: Skip lambda-scoped columns that are only defined in subqueries
                # exists_N_item, select_N_item, where_N_item are created within EXISTS/SELECT/WHERE
                # subqueries and are not available in the parent CTE's scope
                if re.match(r'^(exists|select|where)_\d+_item$', ref):
                    logger.debug(
                        f"SP-110: Skipping lambda-scoped column '{ref}' from propagation"
                    )
                    continue

                # SP-110 Phase 3: Skip if this column is already in qualified_item_columns
                # (we'll add the qualified version instead)
                already_qualified = any(q.endswith(f".{ref}") for q in qualified_item_columns)
                if already_qualified:
                    logger.debug(
                        f"SP-110 Phase 3: Skipping '{ref}' as it's already qualified in '{qualified_item_columns}'"
                    )
                    continue

                # Check if this column is from the source CTE
                # If the expression references just "name_item", it means the source CTE has it
                propagated_item_columns.add(ref)

            # SP-110 Phase 2: If property references exist and we have name_item, include name_item
            # This handles cases like subsetOf/supersetOf that access properties on collection items
            if property_refs and 'name_item' in item_refs:
                logger.debug(
                    f"SP-110 Phase 2: Including name_item for property access: {property_refs}"
                )
                propagated_item_columns.add('name_item')

            if propagated_item_columns or qualified_item_columns:
                logger.info(
                    f"SP-105 Phase 2: Propagating _item columns from expression: "
                    f"unqualified={propagated_item_columns}, qualified={qualified_item_columns}"
                )

        # SP-110-006: Auto-preserve columns from previous CTEs
        # When the fragment expression doesn't explicitly reference columns from a previous
        # CTE but those columns should be propagated (e.g., when the translator incorrectly
        # sets source_table to "resource" instead of the previous CTE), add them here.
        if auto_preserve_columns:
            for col_name in auto_preserve_columns:
                # Skip if already added via propagated_item_columns or qualified_item_columns
                if col_name in propagated_item_columns or any(q.endswith(f".{col_name}") for q in qualified_item_columns):
                    continue
                # Add the column from the source CTE
                qualified_col = f"{source_table}.{col_name}"
                columns.append(qualified_col)
                propagated_item_columns.add(col_name)
                logger.info(
                    f"SP-110-006: Auto-preserving column '{qualified_col}' from previous CTE"
                )

        # Track which columns we've already added to avoid duplicates
        added_columns = set()

        # SP-105: Include the collection item column in SELECT if we need to preserve it
        # This fixes "Referenced column 'name_item' not found" after take()/first()/last()
        if preserve_item_column and item_column_name:
            # Add the item column to SELECT so subsequent operations can reference it
            # Also alias it as 'result' so the standard result column is available
            # SP-110 Phase 3: Qualify the column with source_table if not already qualified
            if '.' not in item_column_name:
                qualified_item_col = f"{source_table}.{item_column_name}"
            else:
                qualified_item_col = item_column_name

            # SP-110 Phase 3: Preserve the column with BOTH the original name AND 'result' alias
            # This allows subsequent operations to reference it as cte_N.name_item
            # while also providing the standard 'result' column
            columns.append(f"{qualified_item_col} AS {item_column_name}")
            added_columns.add(item_column_name)

            # Also add the 'result' alias if different from item_column_name
            if result_alias != item_column_name:
                columns.append(f"{qualified_item_col} AS {result_alias}")
                added_columns.add(result_alias)

            logger.info(
                f"SP-105: Preserved collection item column '{qualified_item_col}' as '{item_column_name}' and '{result_alias}'"
            )
        else:
            # Check if fragment has explicit preserved_columns list (for combine/exclude)
            if hasattr(fragment, 'preserved_columns') and fragment.preserved_columns:
                # Add all preserved columns to SELECT (deduplicate)
                for col_name in set(fragment.preserved_columns):
                    # Skip if already added
                    if col_name in added_columns:
                        continue
                    # Qualify column with source_table if not already qualified
                    if '.' not in col_name and not col_name.startswith(source_table + '.'):
                        qualified_col = f"{source_table}.{col_name}"
                    else:
                        qualified_col = col_name
                    columns.append(qualified_col)
                    added_columns.add(col_name)
                    logger.info(
                        f"SP-105 Phase 2: Preserving column '{qualified_col}' from preserved_columns list"
                    )

            # SP-110 Phase 3: Add qualified column references (from previous CTEs)
            # These are already fully qualified (e.g., cte_2.name_item) and should be used as-is
            for qualified_col in qualified_item_columns:
                # Extract just the column name for tracking
                col_name = qualified_col.split('.')[-1]
                # Skip if already added (check by column name, not full qualified reference)
                if col_name in added_columns:
                    continue
                # Use the fully qualified reference directly
                columns.append(qualified_col)
                added_columns.add(col_name)
                logger.info(
                    f"SP-110 Phase 3: Preserving qualified column '{qualified_col}' from previous CTE"
                )

            # Add propagated _item columns from source CTEs (deduplicate)
            for col_name in propagated_item_columns:
                # Skip if already added (either directly or via preserved_columns)
                if col_name in added_columns:
                    continue
                qualified_col = f"{source_table}.{col_name}"
                columns.append(qualified_col)
                added_columns.add(col_name)
                logger.info(
                    f"SP-105 Phase 2: Propagating column '{qualified_col}' from source CTE"
                )

            # Standard case: add the expression as result
            # SP-110-006: Special handling for repeat() expressions
            # The repeat() function returns a complete RECURSIVE CTE expression as a subquery
            # We need to wrap it properly and extract the repeat_elem_N column
            if is_repeat_expression and repeat_elem_column:
                # The expression is a complete RECURSIVE CTE, use it as a subquery
                # Extract the repeat_elem_N column for subsequent operations
                columns.append(f"{expression} AS {result_alias}")
                # Also add the repeat_elem_N column directly if this is the first time
                # This allows subsequent UNNEST operations to access the elements
                # The subquery returns a JSON array, so we need to extract elements
                # For now, just add the result column - the UNNEST will handle the array
                logger.info(
                    f"SP-110-006: Wrapped repeat() expression as subquery with result alias '{result_alias}'"
                )
            else:
                columns.append(f"{expression} AS {result_alias}")

        query = (
            "SELECT "
            + ", ".join(columns) + "\n"
            f"FROM {source_table}"
        )

        # SP-025-003: Add GROUP BY clause for aggregate functions
        # When fragment.is_aggregate=True (e.g., COUNT(*), SUM(), etc.),
        # all non-aggregated columns must be in GROUP BY clause
        # SP-108-003: Check if order columns should be excluded from GROUP BY
        exclude_order = fragment.metadata.get("exclude_order_from_group_by", False)
        if fragment.is_aggregate and (ordering_columns or source_table.startswith("cte_")):
            # Build GROUP BY with id, resource, and optionally ordering columns
            group_by_columns = [id_column]

            # Include resource in GROUP BY
            if source_table == "resource":
                group_by_columns.append("resource")
            elif source_table and not source_table.startswith("cte_"):
                group_by_columns.append(f"{source_table}.resource")
            else:
                group_by_columns.append(f"{source_table}.resource")

            # SP-108-003: Include ordering columns in GROUP BY only if not excluded
            # Functions like all() aggregate across all elements per patient, not per row
            if not exclude_order:
                group_by_columns.extend(ordering_columns)

            query += f"\nGROUP BY {', '.join(group_by_columns)}"

        # SP-022-004: Add WHERE clause for row filtering (first/last/skip/take)
        if fragment.metadata.get("filter"):
            query += f"\nWHERE {fragment.metadata['filter']}"

        # SP-022-012: Add WHERE clause for where() function filtering on unnested collections
        where_filter = fragment.metadata.get("where_filter")
        if where_filter:
            query += f"\nWHERE {where_filter}"

        # SP-022-004: Handle subset filters (first/last/skip/take on unnested collections)
        subset_filter = fragment.metadata.get("subset_filter")
        if subset_filter and ordering_columns:
            filter_clause = self._build_subset_filter(
                subset_filter,
                ordering_columns,
                fragment.metadata.get("subset_count"),
                source_table,  # Pass source_table for subquery-based filters
            )
            if filter_clause:
                query += f"\nWHERE {filter_clause}"

        return query

    def _build_subset_filter(
        self,
        filter_type: str,
        ordering_columns: List[str],
        count: Optional[int] = None,
        source_table: Optional[str] = None,
    ) -> str:
        """Build WHERE clause for subset operations on unnested collections.

        Args:
            filter_type: Type of filter ("first", "last", "skip", "take")
            ordering_columns: List of ordering column names from UNNEST CTEs
            count: Optional count for skip/take operations
            source_table: Source table/CTE name for subquery-based filters

        Returns:
            WHERE clause condition string
        """
        if not ordering_columns:
            return ""

        if filter_type == "first":
            # For first(), all ordering columns must equal 1
            conditions = [f"{col} = 1" for col in ordering_columns]
            return " AND ".join(conditions)

        elif filter_type == "last":
            # For last(), we need the last row per patient when sorted by ordering columns.
            # SP-022-004: Use correlated subquery to find per-patient last element.
            #
            # For Patient.name.given.last():
            # - For each patient, find the last given name
            # - This is the row where (ordering columns) match the max within that patient
            if not source_table:
                return ""

            if len(ordering_columns) == 1:
                # Single level: find max per patient using correlated subquery
                last_col = ordering_columns[0]
                # Correlate by patient id
                return (
                    f"{last_col} = (SELECT MAX(sub.{last_col}) FROM {source_table} sub "
                    f"WHERE sub.id = {source_table}.id)"
                )
            else:
                # Multiple levels: find the last row per patient by ordering columns
                # We need the row where all ordering columns match the max values
                # for that patient when sorted by all columns DESC.
                order_clause = ", ".join([f"sub.{col} DESC" for col in ordering_columns])
                conditions = []
                for col in ordering_columns:
                    conditions.append(
                        f"{col} = (SELECT sub.{col} FROM {source_table} sub "
                        f"WHERE sub.id = {source_table}.id "
                        f"ORDER BY {order_clause} LIMIT 1)"
                    )
                return " AND ".join(conditions)

        elif filter_type == "skip" and count is not None:
            # For skip(n), the first ordering column must be > n
            if ordering_columns:
                return f"{ordering_columns[0]} > {count}"
            return ""

        elif filter_type == "take" and count is not None:
            # For take(n), the first ordering column must be <= n
            if ordering_columns:
                return f"{ordering_columns[0]} <= {count}"
            return ""

        return ""

    def _wrap_unnest_query(
        self,
        fragment: SQLFragment,
        source_table: str,
        cte_name: str,
        ordering_columns: List[str],
    ) -> tuple[str, str]:
        """Wrap a fragment that requires array unnesting in a SELECT statement.

        Population-first array navigation leverages LATERAL UNNEST to flatten JSON
        arrays while preserving the patient or resource identifier from the source
        table. This helper extracts the required metadata from the fragment,
        delegates UNNEST syntax generation to the active dialect, and returns a
        complete SELECT statement suitable for inclusion in a CTE.

        ARRAY ORDERING FIX (SP-020-DEBUG):
            Adds ROW_NUMBER() OVER (...) to preserve array element ordering across
            nested LATERAL UNNEST operations. Uses PARTITION BY with previous ordering
            columns to maintain correct sequence.

        Args:
            fragment: SQL fragment flagged with ``requires_unnest=True`` and carrying
                UNNEST metadata (``array_column``, optional aliases).
            source_table: Name of the table or CTE that provides the array column.
            cte_name: Name of the CTE being generated (used for order column naming).
            ordering_columns: List of ordering columns from previous UNNESTs.

        Returns:
            Tuple of (SQL SELECT statement, order column name)

        Raises:
            ValueError: If the fragment is not marked for UNNEST processing, if the
                source table is missing, or if required metadata is absent.
            AttributeError: If the active dialect does not implement
                ``generate_lateral_unnest``.
        """
        if not fragment.requires_unnest:
            raise ValueError(
                "_wrap_unnest_query expects fragment.requires_unnest to be True"
            )

        source = source_table.strip()
        if not source:
            raise ValueError("source_table must be a non-empty string for UNNEST queries")

        expression = fragment.expression.strip()
        if expression.upper().startswith("SELECT"):
            # Can't easily add ordering to pre-built SELECT, return as-is
            return expression, None

        metadata = fragment.metadata or {}
        array_column_raw = metadata.get("array_column")
        if not array_column_raw:
            raise ValueError(
                "SQLFragment metadata must contain 'array_column' for UNNEST operations"
            )

        array_column = array_column_raw.strip()
        result_alias = (metadata.get("result_alias") or "item").strip()
        if not result_alias:
            raise ValueError("result_alias metadata cannot be empty when provided")

        # FIX: When source_table is not "resource" and not a CTE, it means we're
        # selecting from an actual resource table (e.g., "Patient", "Observation").
        # These tables have a "resource" column containing JSON data, not an "id" column.
        # We need to extract the id from the resource column.
        if source and source != "resource" and not source.startswith("cte_"):
            # Use dialect's extract_json_string method to get id from resource column
            id_column = (metadata.get("id_column") or
                f"{self.dialect.extract_json_string(f'{source}.resource', '$.id')}").strip()
        else:
            # Original logic for "resource" placeholder or CTEs
            id_column = (metadata.get("id_column") or f"{source}.id").strip()

        if not id_column:
            raise ValueError("id_column metadata cannot be empty when provided")

        # SP-025-003 FIX: When source is an actual resource type (not "resource" placeholder),
        # qualify bare "resource" references in array_column expression.
        # This handles cases like Patient.name.empty() where SQL generator updates source to "Patient",
        # but array_column expressions still contain json_extract(resource, ...) which should be
        # json_extract(Patient.resource, ...) to reference the correct column.
        if source and source != "resource" and not source.startswith("cte_"):
            # Pattern matches 'resource' as a standalone column reference in json_* functions:
            # - Not preceded by a dot (would be table.resource already)
            # - Not preceded by alphanumeric (would be part of another name)
            # - Followed by comma, closing paren, or end of string
            array_column = re.sub(
                r'(?<![\w.])resource(?=[\s,)])',
                f'{source}.resource',
                array_column
            )

        # SP-022-017: Fix table alias propagation for chained path functions
        # When a fragment has from_element_column=True, it means the UNNEST is
        # extracting from a result column (e.g., from first().given) rather than
        # from the original resource. In this case, the 'result' column reference
        # in the array_column expression needs to be qualified with the source
        # table name to ensure proper column resolution in the CTE chain.
        if metadata.get("from_element_column") and source.startswith("cte_"):
            # Qualify 'result' column reference with the source table name
            # This transforms json_extract(result, '$.given[*]')
            # into json_extract(cte_2.result, '$.given[*]')
            # Pattern matches 'result' as a standalone column reference:
            # - Not preceded by a dot (would be table.result already)
            # - Not preceded by alphanumeric (would be part of another name)
            # - Followed by comma, closing paren, or end of string
            array_column = re.sub(
                r'(?<![.\w])result(?=[\s,)])',
                f'{source}.result',
                array_column
            )

        generate_lateral_unnest = getattr(self.dialect, "generate_lateral_unnest", None)
        if generate_lateral_unnest is None:
            raise AttributeError(
                f"{self.dialect.__class__.__name__} must implement "
                "'generate_lateral_unnest' for UNNEST operations"
            )

        prepared_array = self.dialect.prepare_unnest_source(array_column)
        unnest_clause = generate_lateral_unnest(source, prepared_array, result_alias)

        projection_expression_raw = metadata.get("projection_expression")
        if projection_expression_raw:
            projected_column = projection_expression_raw.strip()
        else:
            projected_column = result_alias

        # SP-110-008 FIX: Always ensure "result" column is available in UNNEST CTE output
        # When projected_column differs from result_alias (e.g., a projection expression),
        # we need to ensure both the item column AND the result column are available.
        # The result column must be LAST (test runner extracts row[-1]).
        if projected_column == result_alias:
            # Simple case: result_alias is the item name (e.g., "given_item")
            # Output: "given_item AS given_item, given_item AS result"
            # This makes both columns available: given_item for subsequent path access,
            # and result for the final SELECT/WHERE clause.
            select_projection = f"{result_alias} AS {result_alias}, {result_alias} AS result"
        else:
            # Complex projection expression (e.g., json_extract_string(given_item, '$.family'))
            # Output: "json_extract_string(...) AS given_item, json_extract_string(...) AS result"
            select_projection = f"{projected_column} AS {result_alias}, {projected_column} AS result"

        # SP-020-DEBUG: Add ROW_NUMBER() to preserve array ordering
        order_column = f"{cte_name}_order"

        # Build ROW_NUMBER() clause with PARTITION BY
        # SP-022-004: Always partition by id for population-first semantics
        # This ensures first()/last()/etc. operate per-patient, not globally
        if ordering_columns:
            # Partition by id and previous ordering columns
            partition_cols = [id_column] + ordering_columns
            partition_by = "PARTITION BY " + ", ".join(partition_cols)
            row_number_clause = f"ROW_NUMBER() OVER ({partition_by}) AS {order_column}"
        else:
            # First UNNEST: partition by id for population-first semantics
            row_number_clause = f"ROW_NUMBER() OVER (PARTITION BY {id_column}) AS {order_column}"

        # Build column list: id, resource, ordering columns, new order, result (MUST BE LAST)
        # Test runner extracts row[-1], so result must be the final column
        columns = [id_column]

        # FIX: Include resource column to propagate it through CTE chain
        # This fixes "Referenced column 'resource' not found" errors
        if source == "resource":
            # First CTE selecting from resource table
            columns.append("resource")
        elif source and not source.startswith("cte_"):
            # Selecting from some other table (not a CTE)
            columns.append(f"{source}.resource")
        else:
            # Selecting from a previous CTE - pass through resource if it has it
            columns.append(f"{source}.resource")

        columns.extend(ordering_columns)
        columns.append(row_number_clause)  # New ordering column
        columns.append(select_projection)  # Result MUST be last

        return (
            "SELECT "
            + ", ".join(columns) + "\n"
            f"FROM {source}, "
            f"{unnest_clause}"
        ), order_column

    # === Methods from CTEAssembler ===

    def _assemble_query(self, ctes: List[CTE]) -> str:
        """Combine the provided CTEs into a single SQL query string.

        The method orchestrates the three assembler stages:
            1. Order CTEs according to dependency requirements
            2. Generate the WITH clause that defines each CTE
            3. Append the final SELECT that exposes the terminal CTE

        ARRAY ORDERING FIX (SP-020-DEBUG):
            Collects ordering columns from UNNEST CTEs and adds ORDER BY clause
            to preserve array element ordering.

        COLLECTION AGGREGATION FIX (SP-022-001):
            When the CTE chain contains UNNEST operations and ends with an
            aggregate function (count, empty, exists), adds a final aggregation
            CTE to produce scalar results instead of per-row results.

        Args:
            ctes: Ordered list of `CTE` objects produced by _build_cte_chain.

        Returns:
            Complete SQL query string ready for execution.

        Raises:
            ValueError: If the CTE list is empty or contains invalid entries.
        """
        self._validate_cte_collection(ctes)

        ordered_ctes = self._order_ctes_by_dependencies(ctes)

        # SP-020-DEBUG: Collect ordering columns from CTEs for ORDER BY
        ordering_columns = []
        has_unnest = False
        for cte in ordered_ctes:
            if "order_column" in cte.metadata:
                ordering_columns.append(cte.metadata["order_column"])
            if cte.requires_unnest:
                has_unnest = True
            # SP-022-019: If a CTE has a pre-built SELECT statement (like select()),
            # it breaks the ordering chain since those columns won't be propagated.
            if cte.source_fragment and cte.source_fragment.expression.strip().upper().startswith("SELECT"):
                ordering_columns.clear()
            # SP-108-003: If a CTE has exclude_order_from_group_by metadata (like all()),
            # it breaks the ordering chain since order columns are not in the output.
            if cte.source_fragment and cte.source_fragment.metadata.get("exclude_order_from_group_by", False):
                ordering_columns.clear()

        # SP-022-001: Check if we need collection aggregation
        # When we have UNNEST operations followed by aggregate functions,
        # we need to aggregate the unnested rows into a single result
        final_cte = ordered_ctes[-1]
        needs_aggregation = self._needs_collection_aggregation(
            final_cte, has_unnest, ordering_columns
        )

        if needs_aggregation:
            # Add aggregation CTE to the chain
            ordered_ctes, ordering_columns = self._add_aggregation_cte(
                ordered_ctes, final_cte, ordering_columns
            )

        with_clause = self._generate_with_clause(ordered_ctes)
        final_select = self._generate_final_select(ordered_ctes[-1], ordering_columns)

        if with_clause:
            return f"{with_clause}\n{final_select}"

        return final_select

    def _needs_collection_aggregation(
        self,
        final_cte: CTE,
        has_unnest: bool,
        ordering_columns: List[str],
    ) -> bool:
        """Determine if the CTE chain needs a final aggregation step.

        Collection aggregation is needed when:
        1. There are UNNEST operations in the chain (has_unnest=True)
        2. The final CTE is an aggregate function (count, empty, exists)
        3. SP-108-001: The final CTE is a comparison involving an aggregate function
        4. SP-108-003: The final CTE is a unary operator (not()) on an aggregate function

        This ensures FHIRPath expressions like `Patient.name.given.count()`
        return a single scalar value (5) instead of per-row values (0,0,0,0,0).

        Args:
            final_cte: The last CTE in the chain
            has_unnest: Whether any CTE in the chain has requires_unnest=True
            ordering_columns: List of ordering columns (indicates UNNEST depth)

        Returns:
            True if aggregation is needed, False otherwise
        """
        if not has_unnest and not ordering_columns:
            # No UNNEST operations, no aggregation needed
            return False

        # Check if final CTE is an aggregate function
        function_name = final_cte.metadata.get("function", "")
        aggregate_functions = {"count", "empty", "exists"}

        # SP-108-001: Check if final CTE is a comparison involving an aggregate function
        # For example, count() > 5 should trigger aggregation
        is_comparison = final_cte.metadata.get("is_comparison", False)
        aggregate_function = final_cte.metadata.get("aggregate_function", "")

        # SP-108-003: Check if final CTE is a unary operator (like not()) on an aggregate function
        # For example, empty().not() should trigger aggregation
        is_unary_operator = function_name in {"not", "minus"}
        has_aggregate_operand = aggregate_function in aggregate_functions

        return function_name in aggregate_functions or (is_comparison and aggregate_function in aggregate_functions) or (is_unary_operator and has_aggregate_operand)

    def _add_aggregation_cte(
        self,
        ctes: List[CTE],
        final_cte: CTE,
        ordering_columns: List[str],
    ) -> tuple:
        """Add an aggregation CTE to properly aggregate unnested collection results.

        This method creates a new CTE that aggregates the unnested rows from
        the previous CTE into a single result per resource id.

        For count(): SELECT id, COUNT(*) as result FROM previous_cte GROUP BY id
        For empty(): SELECT id, (COUNT(*) = 0) as result FROM previous_cte GROUP BY id
        For exists(): SELECT id, (COUNT(*) > 0) as result FROM previous_cte GROUP BY id

        Handles comparisons like count() = 5 by extracting the comparison operator
        and value from the source expression.

        Args:
            ctes: Current list of CTEs
            final_cte: The last CTE (which has the aggregate function)
            ordering_columns: List of ordering columns

        Returns:
            Tuple of (updated CTE list, updated ordering columns)
        """
        # SP-108-001: Get the function name - check both direct function metadata
        # and aggregate_function metadata (for comparisons involving aggregates)
        # SP-108-003: Check aggregate_function first for unary operators like not()
        aggregate_function = final_cte.metadata.get("aggregate_function", "")
        function_name = aggregate_function or final_cte.metadata.get("function", "")
        if not function_name:
            function_name = "count"

        # Check if the source expression contains a comparison or unary operator
        # Look for patterns like "= 5", "!= 10", "> 3", "= false", etc.
        source_expr = final_cte.source_expression or ""
        comparison_op, comparison_val = self._extract_comparison_parts(source_expr)

        # SP-108-001: Detect unary operators in source expression
        # Handles expressions like "-count()", "+count()", "-COALESCE(...)"
        unary_operator = self._extract_unary_operator(source_expr)

        # SP-108-003: For not(), we need to negate the aggregate expression
        if final_cte.metadata.get("function") == "not":
            unary_operator = "NOT"

        # SP-108-001: Check if the final CTE itself is a comparison (not the source expression)
        # For example, when we have count() > 5, the final CTE contains the comparison expression
        is_final_cte_comparison = final_cte.metadata.get("is_comparison", False)
        if is_final_cte_comparison and not comparison_op:
            # The final CTE is the comparison, extract from its expression instead
            final_cte_expr = final_cte.query or source_expr
            comparison_op, comparison_val = self._extract_comparison_parts(final_cte_expr)

        # Generate aggregation expression based on function type
        if function_name == "count":
            base_agg = "COUNT(*)"
            # SP-108-001: Apply unary operator if present
            if unary_operator:
                base_agg = f"({unary_operator} {base_agg})"
            if comparison_op:
                agg_expr = f"({base_agg} {comparison_op} {comparison_val})"
            else:
                agg_expr = base_agg
        elif function_name == "empty":
            # empty() returns true if collection is empty
            base_result = "(COUNT(*) = 0)"
            # SP-108-003: Apply NOT operator if present (for not() on empty())
            if unary_operator == "NOT":
                base_result = f"(NOT {base_result})"
            if comparison_op:
                # Handle empty() = true/false comparisons
                agg_expr = f"({base_result} {comparison_op} {comparison_val})"
            else:
                agg_expr = base_result
        elif function_name == "exists":
            # exists() returns true if collection is not empty
            base_result = "(COUNT(*) > 0)"
            # SP-108-003: Apply NOT operator if present (for not() on exists())
            if unary_operator == "NOT":
                base_result = f"(NOT {base_result})"
            if comparison_op:
                # Handle exists() = true/false comparisons
                agg_expr = f"({base_result} {comparison_op} {comparison_val})"
            else:
                agg_expr = base_result
        else:
            agg_expr = "COUNT(*)"

        # Find the CTE before the final one (the one with actual unnested data)
        # We need to aggregate from the CTE that has the unnested rows
        if len(ctes) >= 2:
            source_cte = ctes[-2]  # CTE before the aggregate function CTE
        else:
            source_cte = final_cte

        # Create aggregation CTE
        agg_cte_name = f"cte_{len(ctes) + 1}_agg"
        agg_query = (
            f"SELECT {source_cte.name}.id, "
            f"{source_cte.name}.resource, "
            f"{agg_expr} AS result\n"
            f"FROM {source_cte.name}\n"
            f"GROUP BY {source_cte.name}.id, {source_cte.name}.resource"
        )

        # Determine result type
        result_type = "boolean" if comparison_op else final_cte.metadata.get("result_type", "integer")

        agg_cte = CTE(
            name=agg_cte_name,
            query=agg_query,
            depends_on=[source_cte.name],
            requires_unnest=False,
            metadata={"function": function_name, "result_type": result_type},
        )

        # Replace the final CTE with the aggregation CTE
        # (The original final CTE's expression is wrong for unnested collections)
        new_ctes = ctes[:-1] + [agg_cte]

        # Clear ordering columns since we're aggregating
        return new_ctes, []

    def _extract_comparison_parts(self, source_expr: str) -> tuple:
        """Extract comparison operator and value from a source expression.

        Uses simple string parsing instead of regex for robustness.

        Handles expressions like:
        - "... = 5" -> ("=", "5")
        - "... > 3" -> (">", "3")
        - "... = false" -> ("=", "false")
        - "... = -5" -> ("=", "-5")

        Args:
            source_expr: The source SQL expression

        Returns:
            Tuple of (operator, value) or (None, None) if no comparison found
        """
        if not source_expr:
            return None, None

        # Strip trailing ) and AS result if present
        expr = source_expr.rstrip()
        if expr.endswith(")"):
            expr = expr[:-1].rstrip()

        # Look for comparison operators from the end
        # We search backwards for the last comparison operator
        comparison_ops = [">=", "<=", "!=", "<>", "=", ">", "<"]

        for op in comparison_ops:
            # Find the last occurrence of this operator
            idx = expr.rfind(op)
            if idx == -1:
                continue

            # Extract the value after the operator
            value_part = expr[idx + len(op):].strip()

            # Validate the value is a number or boolean
            # Handle negative numbers, decimals, and booleans
            value_clean = value_part.rstrip(")").strip()

            if not value_clean:
                continue

            # Check if it's a valid value (number or boolean)
            is_valid = False

            # Check for boolean
            if value_clean.lower() in ("true", "false"):
                is_valid = True
            else:
                # SP-108-001: Check for number (including negative and decimal)
                # Also handle unary minus expressions like (- 3)
                try:
                    # Remove any trailing parentheses
                    num_str = value_clean.rstrip(")")
                    # Handle unary minus expressions like (- 3)
                    num_str = num_str.strip()
                    if num_str.startswith("(-"):
                        num_str = "-" + num_str[2:].strip()
                    elif num_str.startswith("(+"):
                        num_str = "+" + num_str[2:].strip()
                    # Remove any remaining parentheses
                    num_str = num_str.rstrip(")")
                    float(num_str)
                    is_valid = True
                    value_clean = num_str
                except ValueError:
                    pass

            if is_valid:
                return op, value_clean

        return None, None

    def _extract_unary_operator(self, source_expr: str) -> Optional[str]:
        """Extract unary operator from a source expression.

        SP-108-001: Detects unary operators (+, -) at the start of expressions.
        Handles expressions like:
        - "-COUNT(*)" -> "-"
        - "+COALESCE(...)" -> "+"
        - "- (5)" -> "-" (operator before parentheses, not inside)

        Args:
            source_expr: The source SQL expression

        Returns:
            The unary operator ("+", "-") or None if no unary operator found
        """
        if not source_expr:
            return None

        # Strip whitespace
        expr = source_expr.strip()

        # Extract the operator by looking at the first non-parenthesis character
        # This handles cases like "- (5)" where operator is outside parens
        operator = None
        for char in expr:
            if char in '+-':
                operator = char
                break
            elif char not in ' \t\n\r(':
                # Not a unary operator, not whitespace, not opening paren
                break

        return operator

    def _order_ctes_by_dependencies(
        self,
        ctes: List[CTE],
        external_tables: Optional[Set[str]] = None
    ) -> List[CTE]:
        """Topologically order CTEs based on their dependency relationships.

        Implements Kahn's algorithm with stable ordering guarantees so CTEs
        that do not depend on each other preserve their input order. The method
        validates the dependency graph before processing, raising informative
        errors when missing references or cycles are detected.

        Args:
            ctes: List of `CTE` objects to order.
            external_tables: Set of external table names (e.g., 'resource') that
                exist in the database context and should not be treated as missing
                CTE dependencies. Defaults to {"resource"}.

        Returns:
            List of CTEs ordered so every dependency appears before its dependents.

        Raises:
            ValueError: If duplicate names, missing dependencies, or dependency
                cycles are detected.
        """
        if not ctes:
            return []

        # Initialize external_tables with default known tables if not provided
        if external_tables is None:
            external_tables = {"resource"}

        cte_map: Dict[str, CTE] = {}
        for cte in ctes:
            if cte.name in cte_map:
                raise ValueError(f"Duplicate CTE name detected: {cte.name}")
            cte_map[cte.name] = cte

        # Check for missing dependencies, excluding external tables
        missing_dependencies: List[str] = []
        seen_missing: Set[str] = set()
        for cte in ctes:
            for dependency in cte.depends_on:
                # Only treat as missing if not in CTEs and not an external table
                if (dependency not in cte_map and
                    dependency not in external_tables and
                    dependency not in seen_missing):
                    seen_missing.add(dependency)
                    missing_dependencies.append(dependency)

        if missing_dependencies:
            missing_list = ", ".join(missing_dependencies)
            raise ValueError(f"Missing CTE dependencies: {missing_list}")

        order_index = {cte.name: index for index, cte in enumerate(ctes)}
        adjacency: Dict[str, List[str]] = {name: [] for name in cte_map}
        indegree: Dict[str, int] = {}
        normalized_dependencies: Dict[str, List[str]] = {}

        for cte in ctes:
            unique_dependencies: List[str] = []
            seen: Set[str] = set()
            for dependency in cte.depends_on:
                if dependency in seen:
                    continue
                seen.add(dependency)
                unique_dependencies.append(dependency)
                # Only build adjacency edges for CTE-to-CTE dependencies
                if dependency in cte_map:
                    adjacency[dependency].append(cte.name)

            normalized_dependencies[cte.name] = unique_dependencies
            # Count only CTE dependencies for indegree (external tables don't count)
            cte_deps_count = sum(1 for dep in unique_dependencies if dep in cte_map)
            indegree[cte.name] = cte_deps_count

        ready_heap: List[tuple[int, str]] = []
        for cte in ctes:
            if indegree[cte.name] == 0:
                heapq.heappush(ready_heap, (order_index[cte.name], cte.name))

        ordered_names: List[str] = []
        while ready_heap:
            _, name = heapq.heappop(ready_heap)
            ordered_names.append(name)

            for dependent in adjacency[name]:
                indegree[dependent] -= 1
                if indegree[dependent] == 0:
                    heapq.heappush(ready_heap, (order_index[dependent], dependent))

        if len(ordered_names) != len(ctes):
            cycle_nodes = {name for name, degree in indegree.items() if degree > 0}
            cycle_path = self._find_dependency_cycle(
                normalized_dependencies,
                cycle_nodes,
                order_index,
            )
            if cycle_path:
                cycle_repr = " -> ".join(cycle_path)
                raise ValueError(f"CTE dependency cycle detected: {cycle_repr}")
            raise ValueError("CTE dependency cycle detected")

        return [cte_map[name] for name in ordered_names]

    def _find_dependency_cycle(
        self,
        dependencies: Dict[str, List[str]],
        candidates: Set[str],
        order_index: Dict[str, int],
    ) -> List[str]:
        """Locate a dependency cycle for diagnostic messaging."""
        visited: Set[str] = set()
        stack: List[str] = []
        stack_positions: Dict[str, int] = {}

        def dfs(node: str) -> Optional[List[str]]:
            visited.add(node)
            stack_positions[node] = len(stack)
            stack.append(node)

            for dependency in dependencies.get(node, []):
                if dependency not in candidates:
                    continue
                if dependency not in visited:
                    result = dfs(dependency)
                    if result:
                        return result
                elif dependency in stack_positions:
                    start_index = stack_positions[dependency]
                    return stack[start_index:] + [dependency]

            stack.pop()
            stack_positions.pop(node, None)
            return None

        ordered_candidates = sorted(
            candidates,
            key=lambda name: order_index.get(name, float("inf")),
        )

        for candidate in ordered_candidates:
            if candidate in visited:
                continue
            result = dfs(candidate)
            if result:
                return result

        return []

    def _generate_with_clause(self, ctes: List[CTE]) -> str:
        """Create the formatted WITH clause for the provided CTE collection.

        The method produces a population-scale ready WITH clause that keeps
        consistent indentation and comma placement across all database
        implementations. Output example:

        ```
        WITH
          cte_1 AS (
            SELECT id FROM patient_resources
          ),
          cte_2 AS (
            SELECT cte_1.id FROM cte_1
          )
        ```

        Args:
            ctes: Ordered list of CTEs that will be defined in the WITH clause.

        Returns:
            String containing the complete WITH clause or an empty string when
            no CTEs are provided (should not occur due to prior validation).
        """
        if not ctes:
            return ""

        formatted_blocks = [
            self._format_cte_definition(cte, index == len(ctes) - 1)
            for index, cte in enumerate(ctes)
        ]
        return "\n".join(["WITH"] + formatted_blocks)

    def _format_cte_definition(self, cte: CTE, is_last: bool) -> str:
        """Format a single CTE definition for inclusion in the WITH clause."""
        normalized_query = self._normalize_query_body(cte.query)
        indented_query = self._indent_query_body(normalized_query)

        closing_line = "  )" if is_last else "  ),"
        if indented_query:
            return "\n".join(
                [
                    f"  {cte.name} AS (",
                    indented_query,
                    closing_line,
                ]
            )

        return "\n".join([f"  {cte.name} AS (", closing_line])

    @staticmethod
    def _normalize_query_body(query: str) -> str:
        """Remove extraneous leading/trailing whitespace from a query body."""
        if not query:
            return ""
        return inspect.cleandoc(query).strip()

    @staticmethod
    def _indent_query_body(query: str) -> str:
        """Indent each query line by four spaces for WITH clause readability."""
        if not query:
            return ""

        return "\n".join(
            f"    {line.rstrip()}" if line else ""
            for line in query.splitlines()
        )

    def _generate_final_select(
        self, final_cte: CTE, ordering_columns: Optional[List[str]] = None
    ) -> str:
        """Generate the terminal SELECT statement for the provided CTE.

        ARRAY ORDERING FIX (SP-020-DEBUG):
            Adds ORDER BY clause when ordering columns are present to preserve
            array element ordering from UNNEST operations.

        SP-104-001: Only add WHERE result IS NOT NULL if the result column exists.
            Not all CTEs create a result column, so we must check before adding
            the WHERE clause to avoid "column not found" errors.

        Example:
            For a final CTE named ``cte_final`` the method returns::

                SELECT * FROM cte_final;

            With ordering columns::

                SELECT * FROM cte_final ORDER BY cte_1_order, cte_2_order;

        Args:
            final_cte: The last CTE in the ordered chain whose results should be
                returned to the caller.
            ordering_columns: Optional list of ordering column names for ORDER BY.

        Returns:
            Final SELECT statement referencing the provided CTE.
        """
        select_statement = f"SELECT * FROM {final_cte.name}"

        # SP-103-007: Filter out NULL results to represent empty collections
        # This handles cases like {} = {} which should return empty results
        # SP-104-001: Only add WHERE clause if result column exists in CTE
        # Check if the CTE query creates a result column by looking for:
        # 1. Explicit " AS result" alias
        # 2. result in the column list
        has_result_column = (
            " AS result" in final_cte.query or 
            " AS  result" in final_cte.query or  # double space handling
            ",result" in final_cte.query.replace(" ", "") or  # comma-separated
            final_cte.query.strip().endswith("result")
        )
        
        if has_result_column:
            select_statement += " WHERE result IS NOT NULL"

        # SP-020-DEBUG: Add ORDER BY if ordering columns are present
        if ordering_columns:
            order_by_clause = "ORDER BY " + ", ".join(ordering_columns)
            select_statement = f"{select_statement} {order_by_clause}"

        select_statement += ";"
        # Ensure the assembled SQL terminates cleanly for direct execution.
        return select_statement

    def _validate_cte_collection(self, ctes: List[CTE]) -> None:
        """Validate the input CTE collection prior to assembly.

        Ensures the list is non-empty and that each element is an instance of
        `CTE`. Additional validation hooks will be added as dependency ordering
        logic becomes more sophisticated in later phases.

        Args:
            ctes: Collection of CTEs that will be assembled.

        Raises:
            ValueError: If the collection is empty or contains invalid entries.
        """
        if not ctes:
            raise ValueError("CTEManager requires at least one CTE to assemble a query")

        for index, cte in enumerate(ctes):
            if not isinstance(cte, CTE):
                raise ValueError(
                    f"ctes[{index}] is not a CTE instance: {type(cte)!r}"
                )

    # === Backward compatibility methods ===
    # These methods provide the same interface as the old CTEBuilder and CTEAssembler
    # classes to allow gradual migration.

    def build_cte_chain(self, fragments: List[SQLFragment]) -> List[CTE]:
        """Build CTE chain from fragments (backward compatible with CTEBuilder).

        This method provides the same interface as CTEBuilder.build_cte_chain()
        for backward compatibility during migration.

        Args:
            fragments: Ordered SQL fragments emitted by the translator.

        Returns:
            List of `CTE` instances.
        """
        return self._build_cte_chain(fragments)

    def assemble_query(self, ctes: List[CTE]) -> str:
        """Assemble CTEs into SQL (backward compatible with CTEAssembler).

        This method provides the same interface as CTEAssembler.assemble_query()
        for backward compatibility during migration.

        Args:
            ctes: Ordered list of `CTE` objects.

        Returns:
            Complete SQL query string ready for execution.
        """
        return self._assemble_query(ctes)


# Backward compatibility aliases
# These aliases allow existing code that imports CTEBuilder or CTEAssembler
# to continue working with the new unified CTEManager class.
CTEBuilder = CTEManager
CTEAssembler = CTEManager

